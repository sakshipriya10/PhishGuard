# -*- coding: utf-8 -*-
"""FinalCode.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZzFSoDLohfnHYc1u9pMBHsIPzsSiUcJJ
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
color = sns.color_palette()
import sklearn.metrics as metrics

import warnings
warnings.filterwarnings("ignore")



Default = pd.read_csv("phishing.csv")
Default.head()

Default.shape

Default.describe()

from sklearn.model_selection import train_test_split

X =  Default.drop('class', axis = 1)
y = Default['class']

X_train, X_test,y_train, y_test = train_test_split(X,y,test_size = 0.3,random_state = 21, stratify = y)

print(X_train.shape)
print(X_test.shape)

plt.figure(figsize=(15,15))
heatmap = sns.heatmap(Default.corr(), annot=True)

# Save the figure first
plt.savefig("correlation_heatmap.png", dpi=300, bbox_inches='tight')

# # Then download it
# files.download("correlation_heatmap.png")

# Create the pie chart
plt.figure(figsize=(8, 8))
Default['class'].value_counts().plot(kind='pie', autopct='%1.2f%%')
plt.title("Phishing Count")
plt.ylabel("")  # Remove the default 'class' label

# Save the figure before showing
plt.savefig("phishing_piechart.png", dpi=300, bbox_inches='tight')

# # Download the file
# files.download("phishing_piechart.png")

# Display the chart in notebook
plt.show()

!pip install imblearn
from imblearn.over_sampling import SMOTE

sm = SMOTE(random_state=33, sampling_strategy=0.9)  # Increase ratio to 90%
X_res, y_res = sm.fit_resample(X_train, y_train)

sm = SMOTE(random_state=33, sampling_strategy='auto')
X_res, y_res = sm.fit_resample(X_train, y_train)

from sklearn.preprocessing import MinMaxScaler

# Step 4: Convert X_res to DataFrame with correct column names
X_res = pd.DataFrame(X_res, columns=X.columns)

# Step 5: Normalize X_res using MinMaxScaler
scaler = MinMaxScaler()
X_res_scaled = scaler.fit_transform(X_res)

# Step 6: Ensure X_test has same column order and names, then scale
X_test = pd.DataFrame(X_test, columns=X.columns)  # Enforce column order
X_test_scaled = scaler.transform(X_test)





import numpy as np
from collections import Counter

print("Before SMOTE:", Counter(y_train))  # Check class imbalance

print("After SMOTE:", Counter(y_res))

import pandas as pd

# Assuming X_train was originally a DataFrame with column names
X_res_df = pd.DataFrame(X_res, columns=X_train.columns)

# Convert y_res into a DataFrame as well
y_res_df = pd.DataFrame(y_res, columns=['class'])  # Change 'class' if needed

# Combine both into a single DataFrame
balanced_df = pd.concat([X_res_df, y_res_df], axis=1)

balanced_df.to_csv("balanced_dataset.csv", index=False)

!pip install python-whois

!pip install tldextract

import ipaddress
import re
import urllib.request
from bs4 import BeautifulSoup
import socket
import requests
from googlesearch import search
import whois
from datetime import date, datetime
import time
from dateutil.parser import parse as date_parse
from urllib.parse import urlparse

class FeatureExtraction:
    features = []
    def __init__(self,url):
        self.features = []
        self.url = url
        self.domain = ""
        self.whois_response = ""
        self.urlparse = ""
        self.response = ""
        self.soup = ""

        try:
            self.response = requests.get(url)
            self.soup = BeautifulSoup(response.text, 'html.parser')
        except:
            pass

        try:
            self.urlparse = urlparse(url)
            self.domain = self.urlparse.netloc
        except:
            pass

        try:
            self.whois_response = whois.whois(self.domain)
        except:
            pass




        self.features.append(self.UsingIp())
        self.features.append(self.longUrl())
        self.features.append(self.shortUrl())
        self.features.append(self.symbol())
        self.features.append(self.redirecting())
        self.features.append(self.prefixSuffix())
        self.features.append(self.SubDomains())
        self.features.append(self.Hppts())
        self.features.append(self.DomainRegLen())
        self.features.append(self.Favicon())


        self.features.append(self.NonStdPort())
        self.features.append(self.HTTPSDomainURL())
        self.features.append(self.RequestURL())
        self.features.append(self.AnchorURL())
        self.features.append(self.LinksInScriptTags())
        self.features.append(self.ServerFormHandler())
        self.features.append(self.InfoEmail())
        self.features.append(self.AbnormalURL())
        self.features.append(self.WebsiteForwarding())
        self.features.append(self.StatusBarCust())

        self.features.append(self.DisableRightClick())
        self.features.append(self.UsingPopupWindow())
        self.features.append(self.IframeRedirection())
        self.features.append(self.AgeofDomain())
        self.features.append(self.DNSRecording())
        self.features.append(self.WebsiteTraffic())
        self.features.append(self.PageRank())
        self.features.append(self.GoogleIndex())
        self.features.append(self.LinksPointingToPage())
        self.features.append(self.StatsReport())


     # 1.UsingIp
    def UsingIp(self):
        try:
            ipaddress.ip_address(self.url)
            return -1
        except:
            return 1

    # 2.longUrl
    def longUrl(self):
        if len(self.url) < 54:
            return 1
        if len(self.url) >= 54 and len(self.url) <= 75:
            return 0
        return -1

    # 3.shortUrl
    def shortUrl(self):
        match = re.search('bit\.ly|goo\.gl|shorte\.st|go2l\.ink|x\.co|ow\.ly|t\.co|tinyurl|tr\.im|is\.gd|cli\.gs|'
                    'yfrog\.com|migre\.me|ff\.im|tiny\.cc|url4\.eu|twit\.ac|su\.pr|twurl\.nl|snipurl\.com|'
                    'short\.to|BudURL\.com|ping\.fm|post\.ly|Just\.as|bkite\.com|snipr\.com|fic\.kr|loopt\.us|'
                    'doiop\.com|short\.ie|kl\.am|wp\.me|rubyurl\.com|om\.ly|to\.ly|bit\.do|t\.co|lnkd\.in|'
                    'db\.tt|qr\.ae|adf\.ly|goo\.gl|bitly\.com|cur\.lv|tinyurl\.com|ow\.ly|bit\.ly|ity\.im|'
                    'q\.gs|is\.gd|po\.st|bc\.vc|twitthis\.com|u\.to|j\.mp|buzurl\.com|cutt\.us|u\.bb|yourls\.org|'
                    'x\.co|prettylinkpro\.com|scrnch\.me|filoops\.info|vzturl\.com|qr\.net|1url\.com|tweez\.me|v\.gd|tr\.im|link\.zip\.net', self.url)
        if match:
            return -1
        return 1

    # 4.Symbol@
    def symbol(self):
        if re.findall("@",self.url):
            return -1
        return 1

    # 5.Redirecting//
    def redirecting(self):
        if self.url.rfind('//')>6:
            return -1
        return 1

    # 6.prefixSuffix
    def prefixSuffix(self):
        try:
            match = re.findall('\-', self.domain)
            if match:
                return -1
            return 1
        except:
            return -1

    # 7.SubDomains
    def SubDomains(self):
        dot_count = len(re.findall("\.", self.url))
        if dot_count == 1:
            return 1
        elif dot_count == 2:
            return 0
        return -1

    # 8.HTTPS
    def Hppts(self):
        try:
            https = self.urlparse.scheme
            if 'https' in https:
                return 1
            return -1
        except:
            return 1

    # 9.DomainRegLen
    def DomainRegLen(self):
        try:
            expiration_date = self.whois_response.expiration_date
            creation_date = self.whois_response.creation_date
            try:
                if(len(expiration_date)):
                    expiration_date = expiration_date[0]
            except:
                pass
            try:
                if(len(creation_date)):
                    creation_date = creation_date[0]
            except:
                pass

            age = (expiration_date.year-creation_date.year)*12+ (expiration_date.month-creation_date.month)
            if age >=12:
                return 1
            return -1
        except:
            return -1

    # 10. Favicon
    def Favicon(self):
        try:
            for head in self.soup.find_all('head'):
                for head.link in self.soup.find_all('link', href=True):
                    dots = [x.start(0) for x in re.finditer('\.', head.link['href'])]
                    if self.url in head.link['href'] or len(dots) == 1 or domain in head.link['href']:
                        return 1
            return -1
        except:
            return -1

    # 11. NonStdPort
    def NonStdPort(self):
        try:
            port = self.domain.split(":")
            if len(port)>1:
                return -1
            return 1
        except:
            return -1

    # 12. HTTPSDomainURL
    def HTTPSDomainURL(self):
        try:
            if 'https' in self.domain:
                return -1
            return 1
        except:
            return -1

    # 13. RequestURL
    def RequestURL(self):
        try:
            for img in self.soup.find_all('img', src=True):
                dots = [x.start(0) for x in re.finditer('\.', img['src'])]
                if self.url in img['src'] or self.domain in img['src'] or len(dots) == 1:
                    success = success + 1
                i = i+1

            for audio in self.soup.find_all('audio', src=True):
                dots = [x.start(0) for x in re.finditer('\.', audio['src'])]
                if self.url in audio['src'] or self.domain in audio['src'] or len(dots) == 1:
                    success = success + 1
                i = i+1

            for embed in self.soup.find_all('embed', src=True):
                dots = [x.start(0) for x in re.finditer('\.', embed['src'])]
                if self.url in embed['src'] or self.domain in embed['src'] or len(dots) == 1:
                    success = success + 1
                i = i+1

            for iframe in self.soup.find_all('iframe', src=True):
                dots = [x.start(0) for x in re.finditer('\.', iframe['src'])]
                if self.url in iframe['src'] or self.domain in iframe['src'] or len(dots) == 1:
                    success = success + 1
                i = i+1

            try:
                percentage = success/float(i) * 100
                if percentage < 22.0:
                    return 1
                elif((percentage >= 22.0) and (percentage < 61.0)):
                    return 0
                else:
                    return -1
            except:
                return 0
        except:
            return -1

    # 14. AnchorURL
    def AnchorURL(self):
        try:
            i,unsafe = 0,0
            for a in self.soup.find_all('a', href=True):
                if "#" in a['href'] or "javascript" in a['href'].lower() or "mailto" in a['href'].lower() or not (url in a['href'] or self.domain in a['href']):
                    unsafe = unsafe + 1
                i = i + 1

            try:
                percentage = unsafe / float(i) * 100
                if percentage < 31.0:
                    return 1
                elif ((percentage >= 31.0) and (percentage < 67.0)):
                    return 0
                else:
                    return -1
            except:
                return -1

        except:
            return -1

    # 15. LinksInScriptTags
    def LinksInScriptTags(self):
        try:
            i,success = 0,0

            for link in self.soup.find_all('link', href=True):
                dots = [x.start(0) for x in re.finditer('\.', link['href'])]
                if self.url in link['href'] or self.domain in link['href'] or len(dots) == 1:
                    success = success + 1
                i = i+1

            for script in self.soup.find_all('script', src=True):
                dots = [x.start(0) for x in re.finditer('\.', script['src'])]
                if self.url in script['src'] or self.domain in script['src'] or len(dots) == 1:
                    success = success + 1
                i = i+1

            try:
                percentage = success / float(i) * 100
                if percentage < 17.0:
                    return 1
                elif((percentage >= 17.0) and (percentage < 81.0)):
                    return 0
                else:
                    return -1
            except:
                return 0
        except:
            return -1

    # 16. ServerFormHandler
    def ServerFormHandler(self):
        try:
            if len(self.soup.find_all('form', action=True))==0:
                return 1
            else :
                for form in self.soup.find_all('form', action=True):
                    if form['action'] == "" or form['action'] == "about:blank":
                        return -1
                    elif self.url not in form['action'] and self.domain not in form['action']:
                        return 0
                    else:
                        return 1
        except:
            return -1

    # 17. InfoEmail
    def InfoEmail(self):
        try:
            if re.findall(r"[mail\(\)|mailto:?]", self.soap):
                return -1
            else:
                return 1
        except:
            return -1

    # 18. AbnormalURL
    def AbnormalURL(self):
        try:
            if self.response.text == self.whois_response:
                return 1
            else:
                return -1
        except:
            return -1

    # 19. WebsiteForwarding
    def WebsiteForwarding(self):
        try:
            if len(self.response.history) <= 1:
                return 1
            elif len(self.response.history) <= 4:
                return 0
            else:
                return -1
        except:
             return -1

    # 20. StatusBarCust
    def StatusBarCust(self):
        try:
            if re.findall("<script>.+onmouseover.+</script>", self.response.text):
                return 1
            else:
                return -1
        except:
             return -1

    # 21. DisableRightClick
    def DisableRightClick(self):
        try:
            if re.findall(r"event.button ?== ?2", self.response.text):
                return 1
            else:
                return -1
        except:
             return -1

    # 22. UsingPopupWindow
    def UsingPopupWindow(self):
        try:
            if re.findall(r"alert\(", self.response.text):
                return 1
            else:
                return -1
        except:
             return -1

    # 23. IframeRedirection
    def IframeRedirection(self):
        try:
            if re.findall(r"[<iframe>|<frameBorder>]", self.response.text):
                return 1
            else:
                return -1
        except:
             return -1

    # 24. AgeofDomain
    def AgeofDomain(self):
        try:
            creation_date = self.whois_response.creation_date
            try:
                if(len(creation_date)):
                    creation_date = creation_date[0]
            except:
                pass

            today  = date.today()
            age = (today.year-creation_date.year)*12+(today.month-creation_date.month)
            if age >=6:
                return 1
            return -1
        except:
            return -1

    # 25. DNSRecording
    def DNSRecording(self):
        try:
            creation_date = self.whois_response.creation_date
            try:
                if(len(creation_date)):
                    creation_date = creation_date[0]
            except:
                pass

            today  = date.today()
            age = (today.year-creation_date.year)*12+(today.month-creation_date.month)
            if age >=6:
                return 1
            return -1
        except:
            return -1

    # 26. WebsiteTraffic
    def WebsiteTraffic(self):
        try:
            rank = BeautifulSoup(urllib.request.urlopen("http://data.alexa.com/data?cli=10&dat=s&url=" + url).read(), "xml").find("REACH")['RANK']
            if (int(rank) < 100000):
                return 1
            return 0
        except :
            return -1

    # 27. PageRank
    def PageRank(self):
        try:
            prank_checker_response = requests.post("https://www.checkpagerank.net/index.php", {"name": self.domain})

            global_rank = int(re.findall(r"Global Rank: ([0-9]+)", rank_checker_response.text)[0])
            if global_rank > 0 and global_rank < 100000:
                return 1
            return -1
        except:
            return -1


    # 28. GoogleIndex
    def GoogleIndex(self):
        try:
            site = search(self.url, 5)
            if site:
                return 1
            else:
                return -1
        except:
            return 1

    # 29. LinksPointingToPage
    def LinksPointingToPage(self):
        try:
            number_of_links = len(re.findall(r"<a href=", self.response.text))
            if number_of_links == 0:
                return 1
            elif number_of_links <= 2:
                return 0
            else:
                return -1
        except:
            return -1

    # 30. StatsReport
    def StatsReport(self):
        try:
            url_match = re.search(
        'at\.ua|usa\.cc|baltazarpresentes\.com\.br|pe\.hu|esy\.es|hol\.es|sweddy\.com|myjino\.ru|96\.lt|ow\.ly', url)
            ip_address = socket.gethostbyname(self.domain)
            ip_match = re.search('146\.112\.61\.108|213\.174\.157\.151|121\.50\.168\.88|192\.185\.217\.116|78\.46\.211\.158|181\.174\.165\.13|46\.242\.145\.103|121\.50\.168\.40|83\.125\.22\.219|46\.242\.145\.98|'
                                '107\.151\.148\.44|107\.151\.148\.107|64\.70\.19\.203|199\.184\.144\.27|107\.151\.148\.108|107\.151\.148\.109|119\.28\.52\.61|54\.83\.43\.69|52\.69\.166\.231|216\.58\.192\.225|'
                                '118\.184\.25\.86|67\.208\.74\.71|23\.253\.126\.58|104\.239\.157\.210|175\.126\.123\.219|141\.8\.224\.221|10\.10\.10\.10|43\.229\.108\.32|103\.232\.215\.140|69\.172\.201\.153|'
                                '216\.218\.185\.162|54\.225\.104\.146|103\.243\.24\.98|199\.59\.243\.120|31\.170\.160\.61|213\.19\.128\.77|62\.113\.226\.131|208\.100\.26\.234|195\.16\.127\.102|195\.16\.127\.157|'
                                '34\.196\.13\.28|103\.224\.212\.222|172\.217\.4\.225|54\.72\.9\.51|192\.64\.147\.141|198\.200\.56\.183|23\.253\.164\.103|52\.48\.191\.26|52\.214\.197\.72|87\.98\.255\.18|209\.99\.17\.27|'
                                '216\.38\.62\.18|104\.130\.124\.96|47\.89\.58\.141|78\.46\.211\.158|54\.86\.225\.156|54\.82\.156\.19|37\.157\.192\.102|204\.11\.56\.48|110\.34\.231\.42', ip_address)
            if url_match:
                return -1
            elif ip_match:
                return -1
            return 1
        except:
            return 1

    def getFeaturesList(self):
        return self.features

# Commented out IPython magic to ensure Python compatibility.
#importing required libraries

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
from sklearn import metrics
import warnings
warnings.filterwarnings('ignore')

#Loading data into dataframe

data = pd.read_csv("balanced_dataset.csv")
data.head()

#Shape of dataframe

data.shape

#Information about the dataset

data.info()

# nunique value in columns

data.nunique()

#droping index column

data = data.drop(['Index'],axis = 1)

#description of dataset

data.describe().T

plt.figure(figsize=(15,15))
heatmap = sns.heatmap(data.corr(), annot=True)

# Save the figure first
plt.savefig("correlation_heatmap2.png", dpi=300, bbox_inches='tight')

# # Then download it
# files.download("correlation_heatmap2.png")



# # Create your pairplot
# df = data[['PrefixSuffix-', 'SubDomains', 'HTTPS','AnchorURL','WebsiteTraffic','class']]
# pair_grid = sns.pairplot(data=df, hue="class", corner=True)

# # Save the figure (must be done before plt.show())
# pair_grid.savefig("pairplot.png", dpi=300, bbox_inches='tight')

# # # Download the file
# # files.download("pairplot.png")

# # Optional: Display the plot in notebook
# plt.show()

# Create the pie chart
plt.figure(figsize=(8, 8))
data['class'].value_counts().plot(kind='pie', autopct='%1.2f%%')
plt.title("Phishing Count")
plt.ylabel("")  # Remove the default 'class' label

# Save the figure before showing
plt.savefig("phishing_piechart.png", dpi=300, bbox_inches='tight')

# # Download the file
# files.download("phishing_piechart.png")

# Display the chart in notebook
plt.show()

# Splitting the dataset into dependant and independant fetature

X = data.drop(["class"],axis =1)
y = data["class"]

# Splitting the dataset into train and test sets: 80-20 split

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)
X_train.shape, y_train.shape, X_test.shape, y_test.shape

# Creating holders to store the model performance results
ML_Model = []
accuracy = []
f1_score = []
recall = []
precision = []

#function to call for storing the results
def storeResults(model, a,b,c,d):
  ML_Model.append(model)
  accuracy.append(round(a, 3))
  f1_score.append(round(b, 3))
  recall.append(round(c, 3))
  precision.append(round(d, 3))

from sklearn.neighbors import KNeighborsClassifier

# Create the KNN model (you can change n_neighbors)
knn = KNeighborsClassifier(n_neighbors=3)

# Fit the model on training data
knn.fit(X_train, y_train)



#predicting the target value from the model for the samples
y_train_knn = knn.predict(X_train)
y_test_knn = knn.predict(X_test)

#computing the accuracy,f1_score,Recall,precision of the model performance

acc_train_knn = metrics.accuracy_score(y_train,y_train_knn)
acc_test_knn = metrics.accuracy_score(y_test,y_test_knn)
print("K-Nearest Neighbors : Accuracy on training Data: {:.3f}".format(acc_train_knn))
print("K-Nearest Neighbors : Accuracy on test Data: {:.3f}".format(acc_test_knn))
print()

f1_score_train_knn = metrics.f1_score(y_train,y_train_knn)
f1_score_test_knn = metrics.f1_score(y_test,y_test_knn)
print("K-Nearest Neighbors : f1_score on training Data: {:.3f}".format(f1_score_train_knn))
print("K-Nearest Neighbors : f1_score on test Data: {:.3f}".format(f1_score_test_knn))
print()

recall_score_train_knn = metrics.recall_score(y_train,y_train_knn)
recall_score_test_knn = metrics.recall_score(y_test,y_test_knn)
print("K-Nearest Neighborsn : Recall on training Data: {:.3f}".format(recall_score_train_knn))
print("Logistic Regression : Recall on test Data: {:.3f}".format(recall_score_test_knn))
print()

precision_score_train_knn = metrics.precision_score(y_train,y_train_knn)
precision_score_test_knn = metrics.precision_score(y_test,y_test_knn)
print("K-Nearest Neighbors : precision on training Data: {:.3f}".format(precision_score_train_knn))
print("K-Nearest Neighbors : precision on test Data: {:.3f}".format(precision_score_test_knn))

#computing the classification report of the model

print(metrics.classification_report(y_test, y_test_knn))

training_accuracy = []
test_accuracy = []
# try max_depth from 1 to 20
depth = range(1,20)
for n in depth:
    knn = KNeighborsClassifier(n_neighbors=n)

    knn.fit(X_train, y_train)
    # record training set accuracy
    training_accuracy.append(knn.score(X_train, y_train))
    # record generalization accuracy
    test_accuracy.append(knn.score(X_test, y_test))


#plotting the training & testing accuracy for n_estimators from 1 to 20
plt.plot(depth, training_accuracy, label="training accuracy")
plt.plot(depth, test_accuracy, label="test accuracy")
plt.ylabel("Accuracy")
plt.xlabel("n_neighbors")
plt.legend();

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

y_test_proba_knn = knn.predict_proba(X_test)[:, 1]

# Compute FPR and TPR
fpr_knn, tpr_knn, _ = roc_curve(y_test, y_test_proba_knn)
roc_auc_knn = auc(fpr_knn, tpr_knn)

plt.figure()
plt.plot(fpr_knn, tpr_knn, color='blue', label=f'KNN (AUC = {roc_auc_knn:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.title('ROC Curve - KNN')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.grid()
plt.show()

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Create and train KNN model
knn = KNeighborsClassifier(n_neighbors=10)
knn.fit(X_train, y_train)

# Make predictions
y_test_pred = knn.predict(X_test)

# Generate confusion matrix
cm = confusion_matrix(y_test, y_test_pred)

# Plot 2x2 confusion matrix
plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Predicted Legitimate', 'Predicted Phishing'],
            yticklabels=['Actual Legitimate', 'Actual Phishing'])
plt.title('KNN Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.show()

# Print classification report
from sklearn.metrics import classification_report
print(classification_report(y_test, y_test_pred))

#storing the results. The below mentioned order of parameter passing is important.

storeResults('K-Nearest Neighbors',acc_test_knn,f1_score_test_knn,
             recall_score_train_knn,precision_score_train_knn)

# Import necessary module
# ANN model
from sklearn.neural_network import MLPClassifier

# Instantiate the model
ann = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam', max_iter=200)

# Fit the model
ann.fit(X_train, y_train)



# Predicting the target values from the model for the samples
y_train_ann = ann.predict(X_train)
y_test_ann = ann.predict(X_test)

# Computing the accuracy, f1_score, recall, and precision of the model performance
acc_train_ann = metrics.accuracy_score(y_train, y_train_ann)
acc_test_ann = metrics.accuracy_score(y_test, y_test_ann)
print("Artificial Neural Network : Accuracy on training Data: {:.3f}".format(acc_train_ann))
print("Artificial Neural Network : Accuracy on test Data: {:.3f}".format(acc_test_ann))
print()

f1_score_train_ann = metrics.f1_score(y_train, y_train_ann)
f1_score_test_ann = metrics.f1_score(y_test, y_test_ann)
print("Artificial Neural Network : f1_score on training Data: {:.3f}".format(f1_score_train_ann))
print("Artificial Neural Network : f1_score on test Data: {:.3f}".format(f1_score_test_ann))
print()

recall_score_train_ann = metrics.recall_score(y_train, y_train_ann)
recall_score_test_ann = metrics.recall_score(y_test, y_test_ann)
print("Artificial Neural Network : Recall on training Data: {:.3f}".format(recall_score_train_ann))
print("Artificial Neural Network : Recall on test Data: {:.3f}".format(recall_score_test_ann))
print()

precision_score_train_ann = metrics.precision_score(y_train, y_train_ann)
precision_score_test_ann = metrics.precision_score(y_test, y_test_ann)
print("Artificial Neural Network : Precision on training Data: {:.3f}".format(precision_score_train_ann))
print("Artificial Neural Network : Precision on test Data: {:.3f}".format(precision_score_test_ann))

# Computing the classification report of the model
print(metrics.classification_report(y_test, y_test_ann))

# Lists to store accuracy values
training_accuracy = []
test_accuracy = []

# Try different numbers of hidden layer neurons (fewer values for efficiency)
neurons = range(10, 110, 20)  # Checking only 10, 30, 50, ..., 90

for n in neurons:
    # Instantiate the ANN model with optimizations
    ann = MLPClassifier(hidden_layer_sizes=(n,), activation='relu', solver='sgd',
                        max_iter=200, random_state=42, warm_start=True)

    # Fit the model
    ann.fit(X_train, y_train)

    # Record training set accuracy
    training_accuracy.append(ann.score(X_train, y_train))

    # Record generalization accuracy
    test_accuracy.append(ann.score(X_test, y_test))

# Plot training & testing accuracy for different hidden layer sizes
plt.plot(neurons, training_accuracy, marker='o', linestyle='solid', label="Training Accuracy")
plt.plot(neurons, test_accuracy, marker='s', linestyle='solid', label="Test Accuracy")
plt.ylabel("Accuracy")
plt.xlabel("Number of Neurons in Hidden Layer")
plt.legend()
plt.title("Effect of Hidden Layer Size on ANN Performance")
plt.show()

y_test_proba_ann = ann.predict_proba(X_test)[:, 1]

# Compute ROC
fpr_ann, tpr_ann, _ = roc_curve(y_test, y_test_proba_ann)
roc_auc_ann = auc(fpr_ann, tpr_ann)

plt.figure()
plt.plot(fpr_ann, tpr_ann, color='green', label=f'ANN (AUC = {roc_auc_ann:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.title('ROC Curve - ANN')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.grid()
plt.show()
#this for ann

import numpy as np
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming 'ann' is your trained ANN model and X_train, y_train are defined
# ... and they represent a binary classification problem ...

y_pred = ann.predict(X_train)  # Predict on the training data

# Generate confusion matrix
cm = confusion_matrix(y_train, y_pred)

# Plot the 2x2 confusion matrix as a heatmap with blue boxes
plt.figure(figsize=(6, 4))  # Adjust figure size for 2x2
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Class 0', 'Class 1'],
            yticklabels=['Class 0', 'Class 1'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix of ANN')
plt.show()



# Storing the results (order of parameters must be maintained)
storeResults('Artificial Neural Network', acc_test_ann, f1_score_test_ann,
             recall_score_train_ann, precision_score_train_ann)





# # # Example: Generate a random confusion matrix (replace with your actual data)
# # y_true = np.random.randint(0, 10, size=100)  # True labels (10 classes)
# # y_pred = np.random.randint(0, 10, size=100)  # Predicted labels
# # cm = confusion_matrix(y_true, y_pred)  # Compute confusion matrix (10x10)

# # Plot the heatmap
# plt.figure(figsize=(10, 8))
# sns.heatmap(
#     cm,
#     annot=True,
#     fmt='d',  # Format as integers
#     cmap='Blues',
#     xticklabels=[f'Class {i}' for i in range(10)],
#     yticklabels=[f'Class {i}' for i in range(10)]
# )
# plt.xlabel('Predicted Label')
# plt.ylabel('True Label')
# plt.title('Confusion Matrix Heatmap')
# plt.show()

# Install TensorFlow (if not already installed)
# CNN Model
!pip install tensorflow

# Import necessary libraries
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical
import matplotlib.pyplot as plt

# Load MNIST dataset (handwritten digits 0-9)
(X_train, y_train), (X_test, y_test) = mnist.load_data()

# Normalize pixel values (0-255 → 0-1)
X_train = X_train / 255.0
X_test = X_test / 255.0

# Reshape data for CNN (samples, height, width, channels)
X_train = X_train.reshape(-1, 28, 28, 1)
X_test = X_test.reshape(-1, 28, 28, 1)

# One-hot encode labels (if using categorical_crossentropy)
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

model = Sequential([
    # First Conv-Pool Block
    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),  # 32 filters, 3x3 kernel
    MaxPooling2D((2, 2)),  # Reduces dimensions by half (28x28 → 14x14)

    # Second Conv-Pool Block
    Conv2D(64, (3, 3), activation='relu'),  # 64 filters
    MaxPooling2D((2, 2)),  # 14x14 → 7x7

    # Flatten before Dense layers
    Flatten(),

    # Fully Connected Layers
    Dense(128, activation='relu'),  # 128 neurons
    Dense(10, activation='softmax')  # 10 output neurons (for 0-9 digits)
])

# Compile the model
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Print model summary
model.summary()

history = model.fit(
    X_train, y_train,
    epochs=10,
    batch_size=32,
    validation_data=(X_test, y_test)
)

from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize
import matplotlib.pyplot as plt
import numpy as np

# Predict probabilities
y_score = model.predict(X_test)

# Binarize the true labels
y_test_bin = label_binarize(np.argmax(y_test, axis=1), classes=list(range(10)))

# Initialize plot
plt.figure(figsize=(12, 8))
colors = plt.cm.get_cmap('tab10', 10)

# Plot ROC for each class
for i in range(10):
    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_score[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, color=colors(i), lw=2,
             label=f'Class {i} (AUC = {roc_auc:.2f})')

# Diagonal line for reference
plt.plot([0, 1], [0, 1], 'k--', lw=2)

# Plot formatting
plt.title('ROC Curve for CNN ', fontsize=14)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='lower right')
plt.grid(True)
plt.tight_layout()
plt.show()

# Plot training & validation accuracy
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Evaluate on test data
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc * 100:.2f}%")

y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)  # Convert probabilities to class labels
y_true = np.argmax(y_test, axis=1)  # If y_test is one-hot encoded

# Convert probabilities to class labels (0 or 1)
y_pred = (y_pred_classes> 0.5).astype(int)

# Optional: Flatten in case shape is (n, 1)
y_pred = y_pred.flatten()
y_test = y_test.flatten()  # Flatten the NumPy array directly

# Predict probabilities
y_pred_prob = model.predict(X_test)

# Convert probabilities to class labels
y_pred_phishing = (y_pred_prob > 0.5).astype(int).flatten()

# Flatten ground truth if needed
y_test_phishing = y_test.flatten()

# Generate confusion matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test_phishing, y_pred_phishing)
print(cm)

# Plot heatmap
import seaborn as sns
import matplotlib.pyplot as plt

sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,
            xticklabels=['Class 0', 'Class 1'],
            yticklabels=['Class 0', 'Class 1'])
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix of CNN')
plt.show()

# from sklearn.metrics import confusion_matrix
# import seaborn as sns

# cm = confusion_matrix(y_true, y_pred_classes)

# plt.figure(figsize=(10, 8))
# sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
#             xticklabels=[f'Class {i}' for i in range(10)],
#             yticklabels=[f'Class {i}' for i in range(10)])
# plt.xlabel('Predicted Label')
# plt.ylabel('True Label')
# plt.title('Confusion Matrix Heatmap')
# plt.show()
# Generate confusion matrix

# cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
# sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues')

# plt.figure(figsize=(12, 5))

# # Accuracy plot
# plt.subplot(1, 2, 1)
# plt.plot(history.history['accuracy'], label='Training Accuracy')
# plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
# plt.title('Model Accuracy')
# plt.xlabel('Epochs')
# plt.ylabel('Accuracy')
# plt.legend()

# # Loss plot
# plt.subplot(1, 2, 2)
# plt.plot(history.history['loss'], label='Training Loss')
# plt.plot(history.history['val_loss'], label='Validation Loss')
# plt.title('Model Loss')
# plt.xlabel('Epochs')
# plt.ylabel('Loss')
# plt.legend()

# plt.tight_layout()
# plt.show()

# # Count samples per class
# class_counts = np.sum(y_train, axis=0)  # For one-hot encoded labels

# # Plot pie chart
# plt.figure(figsize=(8, 8))
# plt.pie(class_counts,
#         labels=[str(i) for i in range(10)],
#         autopct='%1.1f%%',
#         startangle=90,
#         colors=sns.color_palette('pastel'))
# plt.title('Class Distribution in Training Data')
# plt.show()

from sklearn.metrics import classification_report
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Generate classification report
report = classification_report(y_true, y_pred_classes, output_dict=True)

# Convert to DataFrame and clean up
report_df = pd.DataFrame(report).transpose()

# Remove 'support' row if present (optional)
report_df = report_df.iloc[:-1, :]  # Excludes last row (support)

# Plot heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(report_df, annot=True, cmap='YlGnBu', fmt='.2f',
            linewidths=0.5, cbar_kws={'shrink': 0.8})
plt.title('Classification Report Heatmap', pad=20)
plt.xticks(rotation=45)
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()



##RNN
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Dense
from sklearn.metrics import accuracy_score

# Drop the Index column and separate features and target
# df = df.drop(columns=['Index'])
X = data.drop(columns=['class'])
y = data['class']

# Normalize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Convert target labels from {-1, 1} to {0, 1}
y = (y == 1).astype(int)

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Reshape input to 3D for RNN: (samples, timesteps, features)
# Treat each sample as a sequence with 1 timestep
X_train_rnn = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))
X_test_rnn = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))

# Build a simple RNN model
model = Sequential([
    SimpleRNN(32, activation='relu', input_shape=(1, X_train.shape[1])),
    Dense(16, activation='relu'),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train_rnn, y_train, epochs=30, batch_size=32, validation_split=0.1, verbose=0)

# Evaluate on test set
y_pred_prob = model.predict(X_test_rnn)
y_pred = (y_pred_prob > 0.5).astype(int)
accuracy = accuracy_score(y_test, y_pred)
accuracy

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Generate and print classification report
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Generate confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=["Class 0", "Class 1"], yticklabels=["Class 0", "Class 1"])
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix for RNN')
plt.show()
print("Accuracy:", accuracy)

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Compute FPR, TPR, and AUC
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
roc_auc = auc(fpr, tpr)

# Plot ROC Curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'RNN (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Diagonal line
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for RNN Model')
plt.legend(loc='lower right')
plt.grid(True)
plt.tight_layout()
plt.show()

##fusion of ANN and CNN
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (Input, Dense, Dropout, Conv2D, MaxPooling2D,
                                    Flatten, concatenate, BatchNormalization,
                                    GlobalAveragePooling2D, LeakyReLU)
from tensorflow.keras.callbacks import (EarlyStopping, ReduceLROnPlateau,
                                      ModelCheckpoint, TensorBoard)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report
import pandas as pd
from sklearn.model_selection import train_test_split

## Data Preparation (Replace with your actual data loading)
def load_enhanced_data(num_samples=5000, img_size=64, num_features=20, num_classes=10):
    # More realistic synthetic data generation
    # Image data with some structure
    X_img = np.random.randn(num_samples, img_size, img_size, 3) * 0.5 + 0.5
    X_img = np.clip(X_img, 0, 1)  # Simulate normalized images

    # Tabular data with meaningful features
    X_tab = np.random.randn(num_samples, num_features)
    # Create some feature relationships
    X_tab[:, 1] = X_tab[:, 0] * 0.7 + np.random.normal(0, 0.1, num_samples)
    X_tab[:, 3] = X_tab[:, 2] * 0.5 - X_tab[:, 1] * 0.3

    # Create meaningful (but synthetic) class relationships
    y = ((X_img.mean(axis=(1,2,3)) * 2 + X_tab[:, 0] - X_tab[:, 1]) * 2).astype(int) % num_classes
    return [X_img, X_tab], tf.keras.utils.to_categorical(y, num_classes=num_classes)

# Load enhanced data
(X_img, X_tab), y = load_enhanced_data()
(X_train_img, X_test_img, X_train_tab, X_test_tab,
 y_train, y_test) = train_test_split(X_img, X_tab, y, test_size=0.3, random_state=42)

## Enhanced Model Architecture
def build_high_performance_fusion(img_shape, tab_shape, num_classes):
    # CNN Branch - Deeper architecture with regularization
    img_input = Input(shape=img_shape, name='image_input')

    x = Conv2D(64, (3,3), padding='same', kernel_regularizer=l2(1e-4))(img_input)
    x = BatchNormalization()(x)
    x = LeakyReLU(alpha=0.1)(x)
    x = MaxPooling2D((2,2))(x)

    x = Conv2D(128, (3,3), padding='same', kernel_regularizer=l2(1e-4))(x)
    x = BatchNormalization()(x)
    x = LeakyReLU(alpha=0.1)(x)
    x = MaxPooling2D((2,2))(x)

    x = Conv2D(256, (3,3), padding='same', kernel_regularizer=l2(1e-4))(x)
    x = BatchNormalization()(x)
    x = LeakyReLU(alpha=0.1)(x)
    x = GlobalAveragePooling2D()(x)

    # ANN Branch - More sophisticated architecture
    tab_input = Input(shape=tab_shape, name='tabular_input')

    y = Dense(128, kernel_regularizer=l2(1e-4))(tab_input)
    y = BatchNormalization()(y)
    y = LeakyReLU(alpha=0.1)(y)
    y = Dropout(0.3)(y)

    y = Dense(64, kernel_regularizer=l2(1e-4))(y)
    y = BatchNormalization()(y)
    y = LeakyReLU(alpha=0.1)(y)
    y = Dropout(0.3)(y)

    # Fusion with attention-like mechanism
    combined = concatenate([x, y])

    z = Dense(256, activation='relu', kernel_regularizer=l2(1e-4))(combined)
    z = BatchNormalization()(z)
    z = Dropout(0.5)(z)

    z = Dense(128, activation='relu', kernel_regularizer=l2(1e-4))(z)
    z = BatchNormalization()(z)
    z = Dropout(0.3)(z)

    output = Dense(num_classes, activation='softmax')(z)

    model = Model(inputs=[img_input, tab_input], outputs=output)
    return model

# Build enhanced model
model = build_high_performance_fusion(
    img_shape=X_train_img.shape[1:],
    tab_shape=X_train_tab.shape[1:],
    num_classes=y_train.shape[1]
)

# Custom optimizer configuration
optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=True)

model.compile(optimizer=optimizer,
              loss='categorical_crossentropy',
              metrics=['accuracy',
                      tf.keras.metrics.Precision(name='precision'),
                      tf.keras.metrics.Recall(name='recall')])

# Enhanced callbacks
callbacks = [
    EarlyStopping(patience=10, restore_best_weights=True, monitor='val_accuracy'),
    ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6),
    ModelCheckpoint('best_fusion_model.h5', save_best_only=True),
    TensorBoard(log_dir='./logs')
]

# Train with data augmentation (simulated)
history = model.fit(
    [X_train_img, X_train_tab], y_train,
    epochs=10,
    batch_size=64,
    validation_data=([X_test_img, X_test_tab], y_test),
    callbacks=callbacks,
    verbose=1
)

## Enhanced Evaluation
def comprehensive_evaluation(model, history, X_test, y_test):
    plt.figure(figsize=(18, 6))

    # Accuracy plot
    plt.subplot(1, 3, 1)
    plt.plot(history.history['accuracy'], label='Train Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title('Accuracy Curves\nFinal Val Acc: {:.2f}%'.format(
        history.history['val_accuracy'][-1]*100), pad=20)
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss plot
    plt.subplot(1, 3, 2)
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Loss Curves\nFinal Val Loss: {:.4f}'.format(
        history.history['val_loss'][-1]), pad=20)
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    # Metrics plot
    plt.subplot(1, 3, 3)
    plt.plot(history.history['precision'], label='Train Precision')
    plt.plot(history.history['val_precision'], label='Val Precision')
    plt.plot(history.history['recall'], label='Train Recall')
    plt.plot(history.history['val_recall'], label='Val Recall')
    plt.title('Precision & Recall Curves')
    plt.xlabel('Epochs')
    plt.ylabel('Score')
    plt.legend()

    plt.tight_layout()
    plt.show()

    # Full evaluation
    print("\n=== Comprehensive Evaluation ===")
    test_loss, test_acc, test_prec, test_rec = model.evaluate(
        [X_test_img, X_test_tab], y_test, verbose=0)
    print(f"Test Accuracy: {test_acc*100:.2f}%")
    print(f"Test Loss: {test_loss:.4f}")
    print(f"Test Precision: {test_prec*100:.2f}%")
    print(f"Test Recall: {test_rec*100:.2f}%")

    # Confusion Matrix with normalization
    y_pred = model.predict([X_test_img, X_test_tab])
    y_pred_classes = np.argmax(y_pred, axis=1)
    y_true = np.argmax(y_test, axis=1)

    cm = confusion_matrix(y_true, y_pred_classes)
    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    plt.figure(figsize=(12, 10))
    sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues',
                xticklabels=[f'Class {i}' for i in range(y_test.shape[1])],
                yticklabels=[f'Class {i}' for i in range(y_test.shape[1])])
    plt.title('Normalized Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

    # Enhanced Classification Report
    report = classification_report(y_true, y_pred_classes, output_dict=True)
    report_df = pd.DataFrame(report).transpose()

    plt.figure(figsize=(12, 6))
    sns.heatmap(report_df.iloc[:-3, :].T, annot=True, cmap='YlGnBu', fmt='.2f',
                cbar_kws={'label': 'Score'})
    plt.title('Detailed Classification Report', pad=20)
    plt.tight_layout()
    plt.show()

# Run comprehensive evaluation
comprehensive_evaluation(model, history, [X_test_img, X_test_tab], y_test)

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# Predict probabilities from model
y_pred = model.predict([X_test_img, X_test_tab])

# Convert one-hot encoded y_test to class labels
y_true = np.argmax(y_test, axis=1)
y_pred_classes = np.argmax(y_pred, axis=1)

# Set your target class (you can change this to any class you want to focus on)
target_class = 1

# Binarize: Class 1 = positive (1), others = negative (0)
y_true_binary = (y_true == target_class).astype(int)
y_pred_binary = (y_pred_classes == target_class).astype(int)

# Compute 2x2 confusion matrix
cm = confusion_matrix(y_true_binary, y_pred_binary)
cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)

# Plot 2x2 confusion matrix
plt.figure(figsize=(6, 5))
sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues',
            xticklabels=['Not Class 1', 'Class 1'],
            yticklabels=['Not Class 1', 'Class 1'])
plt.title('Confusion Matrix of Fusion ')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.tight_layout()
plt.show()

from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize
import matplotlib.pyplot as plt
from itertools import cycle

# Get predicted probabilities
y_score = model.predict([X_test_img, X_test_tab])

# Binarize true labels in case they aren't already
y_test_bin = y_test  # already one-hot encoded in your code
n_classes = y_test.shape[1]

# Compute ROC curve and ROC area for each class
fpr = dict()
tpr = dict()
roc_auc = dict()

for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(y_test_bin.ravel(), y_score.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# Plot all ROC curves
plt.figure(figsize=(10, 8))
colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'red', 'purple', 'brown', 'pink', 'gray', 'olive'])

for i, color in zip(range(n_classes), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=2,
             label=f'Class {i} (AUC = {roc_auc[i]:.2f})')

# Plot micro-average
plt.plot(fpr["micro"], tpr["micro"], color='deeppink', linestyle=':', linewidth=4,
         label=f'Micro-average (AUC = {roc_auc["micro"]:.2f})')

# Plot random baseline
plt.plot([0, 1], [0, 1], 'k--', lw=2)

plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Fusion Model')
plt.legend(loc="lower right")
plt.grid(True)
plt.tight_layout()
plt.show()

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Fix the ANN input features mismatch
y_test_proba_ann = ann.predict_proba(X_test_ann)[:, 1]  # Should match training features of ANN

# Keras models use .predict
y_test_proba_cnn = cnn.predict(X_test_img).ravel()
y_test_proba_rnn = rnn.predict(X_test_seq).ravel()
y_test_proba_fusion = fusion_model.predict([X_test_img, X_test_tab]).ravel()

# Compute ROC and AUC
fpr_ann, tpr_ann, _ = roc_curve(y_test, y_test_proba_ann)
roc_auc_ann = auc(fpr_ann, tpr_ann)

fpr_cnn, tpr_cnn, _ = roc_curve(y_test, y_test_proba_cnn)
roc_auc_cnn = auc(fpr_cnn, tpr_cnn)

fpr_rnn, tpr_rnn, _ = roc_curve(y_test, y_test_proba_rnn)
roc_auc_rnn = auc(fpr_rnn, tpr_rnn)

fpr_fusion, tpr_fusion, _ = roc_curve(y_test, y_test_proba_fusion)
roc_auc_fusion = auc(fpr_fusion, tpr_fusion)

# Plot all ROC curves
plt.figure(figsize=(10, 8))
plt.plot(fpr_ann, tpr_ann, label=f'ANN (AUC = {roc_auc_ann:.2f})', linewidth=2)
plt.plot(fpr_cnn, tpr_cnn, label=f'CNN (AUC = {roc_auc_cnn:.2f})', linewidth=2)
plt.plot(fpr_rnn, tpr_rnn, label=f'RNN (AUC = {roc_auc_rnn:.2f})', linewidth=2)
plt.plot(fpr_fusion, tpr_fusion, label=f'Fusion (AUC = {roc_auc_fusion:.2f})', linewidth=2)

# Random guess line
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')

# Plot formatting
plt.title('Combined ROC Curves - ANN vs CNN vs RNN vs Fusion', fontsize=14)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='lower right')
plt.grid(True)
plt.tight_layout()
plt.show()

